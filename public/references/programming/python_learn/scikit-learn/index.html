<!DOCTYPE html>
<html lang="en-us"><head><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Scikit Learn | yogeshsn</title>
<meta property="og:title" content="Scikit Learn | yogeshsn" />
<meta name="twitter:title" content="Scikit Learn | yogeshsn" />
<meta itemprop="name" content="Scikit Learn | yogeshsn" />
<meta name="application-name" content="Scikit Learn | yogeshsn" />
<meta property="og:site_name" content="" />

<meta name="description" content="">
<meta itemprop="description" content="" />
<meta property="og:description" content="" />
<meta name="twitter:description" content="" />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />



  <meta itemprop="image" content="www.yogeshsn.com.np" />
  <meta property="og:image" content="www.yogeshsn.com.np" />
  <meta name="twitter:image" content="www.yogeshsn.com.np" />
  <meta name="twitter:image:src" content="www.yogeshsn.com.np" />





<meta name="generator" content="Hugo 0.121.1">

    

    <link rel="canonical" href="www.yogeshsn.com.np/references/programming/python_learn/scikit-learn/">
    <link href="/www.yogeshsn.com.np/style.min.d43bc6c79baa87f006efb2b92be952faeedeb1a3ab626c1d6abda52eae049355.css" rel="stylesheet">
    <link href="/www.yogeshsn.com.np/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/www.yogeshsn.com.np/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/www.yogeshsn.com.np/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/www.yogeshsn.com.np/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/www.yogeshsn.com.np/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/www.yogeshsn.com.np/favicon.ico">




<link rel="manifest" href="/www.yogeshsn.com.np/site.webmanifest">

<meta name="msapplication-config" content="/www.yogeshsn.com.np/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/www.yogeshsn.com.np/icons/favicon.svg">

    
    </head>
<body data-theme = "" class="notransition">

<script src="/www.yogeshsn.com.np/js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js" integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="www.yogeshsn.com.np/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title></title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="30" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="www.yogeshsn.com.np/about/yogesh">
                        About me
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="www.yogeshsn.com.np/posts/">
                        Posts
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="www.yogeshsn.com.np/references/">
                        References
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="www.yogeshsn.com.np/categories/">
                        Categories
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Scikit Learn</h1>
                
                
                <div class="post-meta">
                    <time datetime="2023-07-09T13:05:42&#43;05:45" itemprop="datePublished"> Jul 9, 2023 </time>
                </div>
                
            </header>
            
            <div class="page-content">
                <p>Scikit-Learn (also known as sklearn) is a powerful machine learning library in Python. It provides a wide range of algorithms for classification, regression, clustering, and dimensionality reduction. Scikit-Learn is built on top of NumPy, SciPy, and matplotlib, making it easy to use and integrate with other Python libraries.</p>
<p>Some key features of Scikit-Learn include:</p>
<ol>
<li>
<p><strong>Consistent API</strong>: Scikit-Learn provides a consistent and intuitive API, making it easy to switch between different algorithms without changing your code significantly. This consistency simplifies the learning curve and encourages experimentation with various models.</p>
</li>
<li>
<p><strong>Extensive Algorithm Collection</strong>: Scikit-Learn offers a wide range of machine learning algorithms, including:</p>
<ul>
<li>Supervised learning algorithms: Support Vector Machines (SVM), Decision Trees, Random Forests, K-Nearest Neighbors (KNN), and Gradient Boosting.</li>
<li>Unsupervised learning algorithms: K-Means, DBSCAN, and Principal Component Analysis (PCA).</li>
<li>Dimensionality reduction techniques: t-SNE and Singular Value Decomposition (SVD).</li>
</ul>
</li>
<li>
<p><strong>Data Preprocessing</strong>: Scikit-Learn provides numerous tools for data preprocessing, such as data scaling, feature selection, and handling missing values. These preprocessing techniques are crucial for preparing the data before training machine learning models.</p>
</li>
<li>
<p><strong>Model Evaluation</strong>: Scikit-Learn includes functions to assess the performance of machine learning models through various metrics like accuracy, precision, recall, F1-score, and more. It also supports cross-validation techniques to obtain reliable estimates of model performance.</p>
</li>
</ol>
<p>Here&rsquo;s a simple example of how to use Scikit-Learn to train a K-Nearest Neighbors (KNN) classifier on the famous Iris dataset:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load the Iris dataset</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Split the data into training and testing sets</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a KNN classifier</span>
</span></span><span class="line"><span class="cl"><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Train the classifier</span>
</span></span><span class="line"><span class="cl"><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Make predictions on the test set</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span></code></pre></div><p>In this example, we load the Iris dataset, split it into training and testing sets, create a KNN classifier, train it on the training data, and then make predictions on the test data.</p>
<p>Scikit-Learn is widely used in various fields, such as healthcare, finance, marketing, and scientific research. It is a valuable tool for both beginners and experienced data scientists looking to build and deploy machine learning models efficiently.</p>
<p>Certainly. Here are 30 important functions and methods from scikit-learn, along with brief examples. Note that for these examples, I&rsquo;ll assume the following imports:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">model_selection</span><span class="p">,</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">decomposition</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
</span></span></code></pre></div><ol>
<li>
<p>datasets.load_*(): Load built-in datasets</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span></code></pre></div></li>
<li>
<p>model_selection.train_test_split(): Split data into training and test sets</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>preprocessing.StandardScaler(): Standardize features</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>LogisticRegression(): Create and train a logistic regression model</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>model.predict(): Make predictions with a trained model</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>metrics.accuracy_score(): Calculate accuracy of predictions</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>metrics.confusion_matrix(): Compute confusion matrix</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">cm</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>metrics.classification_report(): Generate a classification report</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">report</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>model_selection.cross_val_score(): Perform cross-validation</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">scores</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>model_selection.GridSearchCV(): Perform grid search for hyperparameter tuning</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl"><span class="n">grid_search</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">SVC</span><span class="p">(),</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>preprocessing.OneHotEncoder(): Encode categorical features</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">encoder</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>DecisionTreeClassifier(): Create and train a decision tree</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>RandomForestClassifier(): Create and train a random forest</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>SVC(): Create and train a support vector machine</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>KNeighborsClassifier(): Create and train a k-nearest neighbors classifier</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>KMeans(): Perform K-means clustering</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>decomposition.PCA(): Perform principal component analysis</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">pca</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>preprocessing.MinMaxScaler(): Scale features to a given range</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>model_selection.StratifiedKFold(): Stratified K-Fold cross-validator</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">skf</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</span></span></code></pre></div></li>
<li>
<p>metrics.roc_auc_score(): Compute Area Under the Receiver Operating Characteristic Curve</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">])</span>
</span></span></code></pre></div></li>
<li>
<p>preprocessing.LabelEncoder(): Encode target labels with value between 0 and n_classes-1</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">y_encoded</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>model.feature_importances_: Get feature importances (for tree-based models)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">importances</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">feature_importances_</span>
</span></span></code></pre></div></li>
<li>
<p>metrics.mean_squared_error(): Compute mean squared error</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>metrics.r2_score(): Compute R-squared score</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">r2</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>model_selection.RandomizedSearchCV(): Randomized search for hyperparameter tuning</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span> <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl"><span class="n">random_search</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(),</span> <span class="n">param_dist</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>preprocessing.PolynomialFeatures(): Generate polynomial features</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">poly</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>model_selection.learning_curve(): Generate a learning curve</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">test_scores</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">learning_curve</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">train_sizes</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span></code></pre></div></li>
<li>
<p>metrics.silhouette_score(): Compute the mean Silhouette Coefficient of all samples</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">silhouette_avg</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>decomposition.TruncatedSVD(): Dimensionality reduction using truncated SVD</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">svd</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">X_svd</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>metrics.plot_confusion_matrix(): Plot confusion matrix</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">metrics</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div></li>
</ol>
<p>These functions and methods cover a wide range of machine learning tasks, including data preprocessing, model training, evaluation, hyperparameter tuning, and dimensionality reduction. They form the core of many machine learning workflows in scikit-learn.</p>
<h2 id="bonus-random-forest">Bonus: Random Forest</h2>
<p><strong>Understanding Random Forests</strong></p>
<p>Random Forests are powerful supervised learning algorithms that excel in both classification and regression tasks. They operate by constructing a multitude of decision trees during training, each using a random subset of features and data points. This randomness injects diversity into the forest, preventing overfitting to the training data and enhancing generalization performance on unseen data.</p>
<p><strong>Key Concepts</strong></p>
<ul>
<li><strong>Decision Trees:</strong> These are tree-like structures that recursively split data based on feature values to make predictions.</li>
<li><strong>Ensemble Learning:</strong> Random Forests combine predictions from multiple decision trees (the forest) through majority voting (classification) or averaging (regression) to enhance accuracy and robustness.</li>
<li><strong>Bagging (Bootstrap Aggregation):</strong> This ensemble technique trains each tree on a random sample (with replacement) of the original data, fostering diversity in the forest.</li>
</ul>
<p><strong>Initiation (Using Python&rsquo;s scikit-learn library):</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>  <span class="c1"># For classification tasks</span>
</span></span><span class="line"><span class="cl"><span class="c1"># For regression tasks: from sklearn.ensemble import RandomForestRegressor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a Random Forest Classifier object</span>
</span></span><span class="line"><span class="cl"><span class="n">rf_classifier</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Tune hyperparameters as needed</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Train the model on your features (X) and target variable (y)</span>
</span></span><span class="line"><span class="cl"><span class="n">rf_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span></code></pre></div><p><strong>Common Functions and Hyperparameters:</strong></p>
<ol>
<li><strong>n_estimators (int):</strong> The number of decision trees to create in the forest. More trees generally improve performance, but can increase training time. (Default: 100)</li>
<li><strong>max_depth (int):</strong> The maximum depth allowed for each tree (avoid overfitting with deep trees). (Default: None)</li>
<li><strong>min_samples_split (int):</strong> The minimum number of samples required to split a node in the tree. (Default: 2)</li>
<li><strong>min_samples_leaf (int):</strong> The minimum number of samples allowed in a leaf node. (Default: 1)</li>
<li><strong>max_features (int, &lsquo;auto&rsquo;, &lsquo;sqrt&rsquo;, &rsquo;log2&rsquo;):</strong> The number of features to consider when splitting a node. &lsquo;auto&rsquo; often works well. (Default: &lsquo;auto&rsquo;)</li>
<li><strong>criterion (str):</strong> The function to measure the quality of a split (&lsquo;gini&rsquo; for classification, &lsquo;mse&rsquo; for regression). (Default: &lsquo;gini&rsquo;)</li>
<li><strong>random_state (int):</strong> Controls the randomness in tree selection. Setting it ensures reproducibility. (Default: None)</li>
<li><strong>bootstrap (bool):</strong> Whether to use bagging (True) or not (False). (Default: True)</li>
<li><strong>oob_score (bool):</strong> Whether to compute the out-of-bag (OOB) score (useful for model evaluation). (Default: False)</li>
<li><strong>class_weight (dict, &lsquo;balanced&rsquo;):</strong> Weights assigned to classes (helpful for imbalanced datasets). (Default: None)</li>
</ol>
<p><strong>Applications</strong></p>
<ul>
<li><strong>Classification:</strong> Fraud detection, spam filtering, image recognition, sentiment analysis, credit risk assessment, etc.</li>
<li><strong>Regression:</strong> Stock price prediction, customer churn prediction, real estate pricing, sales forecasting, etc.</li>
</ul>
<p><strong>Example (Classification): Predicting Handwritten Digit Recognition</strong></p>
<p>Imagine you have a dataset of handwritten digits (images) labeled with their corresponding values (0-9). You can use a Random Forest to train a model that can recognize new handwritten digits. Here&rsquo;s a simplified workflow:</p>
<ol>
<li><strong>Load and Preprocess Data:</strong> Load the image data and convert it into suitable features (e.g., pixel intensities).</li>
<li><strong>Split Data:</strong> Divide your data into training and testing sets.</li>
<li><strong>Train the Random Forest:</strong> Create a RandomForestClassifier object and fit it on the training data.</li>
<li><strong>Make Predictions:</strong> Use the trained model to predict the digits on the testing set.</li>
<li><strong>Evaluate Performance:</strong> Calculate metrics like accuracy, precision, recall, and F1-score to assess the model&rsquo;s effectiveness.</li>
</ol>
<p><strong>Advantages</strong></p>
<ul>
<li><strong>High Accuracy and Robustness:</strong> Random Forests often achieve excellent performance on various problems due to their ensemble nature.</li>
<li><strong>Handles Missing Data:</strong> They can handle missing values inherently by considering only available features during splitting.</li>
<li><strong>Feature Importance:</strong> They provide insights into feature importance, aiding in feature selection and understanding model behavior.</li>
<li><strong>Relatively Easy to Use:</strong> Random Forests require less parameter tuning compared to some other algorithms.</li>
</ul>
<p><strong>Disadvantages</strong></p>
<ul>
<li><strong>Can Be Computationally Expensive:</strong> Training large forests with many trees can be time-consuming.</li>
<li><strong>Explainability:</strong> While feature importance offers some insights, individual tree decisions can be complex to interpret.</li>
</ul>
<h2 id="citations">Citations:</h2>
<p>[1] <a href="https://www.linkedin.com/pulse/introduction-scikit-learn-library-python-machine-learning-aritra-pain">https://www.linkedin.com/pulse/introduction-scikit-learn-library-python-machine-learning-aritra-pain</a><br>
[2] <a href="https://zerotomastery.io/blog/how-to-use-scikit-learn/">https://zerotomastery.io/blog/how-to-use-scikit-learn/</a> <br>
[3] <a href="https://www.geeksforgeeks.org/learning-model-building-scikit-learn-python-machine-learning-library/">https://www.geeksforgeeks.org/learning-model-building-scikit-learn-python-machine-learning-library/</a><br>
[4] <a href="https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/">https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/</a><br>
[5] <a href="https://en.wikipedia.org/wiki/Scikit-learn">https://en.wikipedia.org/wiki/Scikit-learn</a></p>
<ul>
<li><strong>Scikit-learn documentation on Random Forest:</strong> Pedregosa, F. et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, 2825-2830.  (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>)</li>
<li><strong>Machine Learning Crash Course by Google:</strong> Google Developers. (n.d.). Machine Learning Crash Course. developers.google.com [Chapter 10: Ensemble Learning]</li>
<li><strong>Introduction to Statistical Learning with Applications in R</strong> by James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). Springer New York. (Chapter 15: Ensemble Methods)</li>
</ul>

            </div>
        </article></main>
</div>


        <hr>
        <div style="text-align: center; font-weight: 800; margin: 20px; color:red">
            <p class="quote" id="daily-quote">
                
            </p>
        </div><footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
<a href="https://x.com/yogesh__sn" target="_blank" rel="noopener noreferrer me"
    title="X">
    <svg viewBox="0 0 1200 1227" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
    <path
        d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z"/>
</svg>
</a>
<a href="https://www.youtube.com/@palpasky6909/videos" target="_blank" rel="noopener noreferrer me"
    title="Youtube">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M22.54 6.42a2.78 2.78 0 0 0-1.94-2C18.88 4 12 4 12 4s-6.88 0-8.6.46a2.78 2.78 0 0 0-1.94 2A29 29 0 0 0 1 11.75a29 29 0 0 0 .46 5.33A2.78 2.78 0 0 0 3.4 19c1.72.46 8.6.46 8.6.46s6.88 0 8.6-.46a2.78 2.78 0 0 0 1.94-2 29 29 0 0 0 .46-5.25 29 29 0 0 0-.46-5.33z">
    </path>
    <polygon points="9.75 15.02 15.5 11.75 9.75 8.48 9.75 15.02"></polygon>
</svg>
</a>
<a href="https://www.linkedin.com/in/yogesh-sn%f0%9f%87%b3%f0%9f%87%b5-1926b8184" target="_blank" rel="noopener noreferrer me"
    title="Linkedin">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
</a>
<a href="https://www.instagram.com/livingonnature" target="_blank" rel="noopener noreferrer me"
    title="Instagram">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect>
    <path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path>
    <line x1="17.5" y1="6.5" x2="17.5" y2="6.5"></line>
</svg>
</a>
</div>
    <small class="footer_copyright">
        © 2024 Yogesh SN.
        
    </small>
</footer><a href="#" title="" id="totop">
    <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" fill="currentColor" stroke="currentColor" viewBox="0 96 960 960">
    <path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197Z"/>
</svg>

</a>


    






    
    <script src="/www.yogeshsn.com.np/js/main.min.35f435a5d8eac613c52daa28d8af544a4512337d3e95236e4a4978417b8dcb2f.js" integrity="sha256-NfQ1pdjqxhPFLaoo2K9USkUSM30&#43;lSNuSkl4QXuNyy8="></script>

    

</body>
</html>
<script>
    
    const quotes = [
"The day a man becomes superior to pleasure he will also be superior to pain - Seneca",
"The only way to do great work is to love what you do - Steve Jobs",
"Do not dwell in the past, do not dream of the future, concentrate the mind on the present moment - Buddha",
"Life is what happens when you're busy making other plans - John Lennon",
"To live is the rarest thing in the world. Most people exist, that is all - Oscar Wilde",
"The greatest glory in living lies not in never falling, but in rising every time we fall - Nelson Mandela",
"In the end, we will remember not the words of our enemies, but the silence of our friends - Martin Luther King Jr.",
"The best way to predict the future is to create it - Peter Drucker",
"You miss 100% of the shots you don't take - Wayne Gretzky",
"I have not failed. I've just found 10,000 ways that won't work - Thomas Edison",
"The only limit to our realization of tomorrow is our doubts of today - Franklin D. Roosevelt",
"The purpose of our lives is to be happy - Dalai Lama",
"Life is really simple, but we insist on making it complicated - Confucius",
"May you live all the days of your life - Jonathan Swift",
"Good friends, good books, and a sleepy conscience: this is the ideal life - Mark Twain",
"To love and be loved is to feel the sun from both sides - David Viscott",
"Keep calm and carry on - British Government, 1939",
"Believe you can and you're halfway there - Theodore Roosevelt",
"Act as if what you do makes a difference. It does - William James",
"Success is not final, failure is not fatal: It is the courage to continue that counts - Winston Churchill",
"Never bend your head. Always hold it high. Look the world straight in the eye - Helen Keller",
"What you get by achieving your goals is not as important as what you become by achieving your goals - Zig Ziglar",
"When one door of happiness closes, another opens; but often we look so long at the closed door that we do not see the one that has been opened for us - Helen Keller",
"It is never too late to be what you might have been - George Eliot",
"You must be the change you wish to see in the world - Mahatma Gandhi",
"If you want to live a happy life, tie it to a goal, not to people or things - Albert Einstein",
"The only impossible journey is the one you never begin - Tony Robbins",
"It does not matter how slowly you go as long as you do not stop - Confucius",
"Everything you’ve ever wanted is on the other side of fear - George Addair",
"Change your thoughts and you change your world - Norman Vincent Peale",
"No one can make you feel inferior without your consent - Eleanor Roosevelt",
"Happiness is not something ready-made. It comes from your own actions - Dalai Lama",
"The best way to find yourself is to lose yourself in the service of others - Mahatma Gandhi",
"The future belongs to those who believe in the beauty of their dreams - Eleanor Roosevelt",
"Don't watch the clock; do what it does. Keep going - Sam Levenson",
"If life were predictable it would cease to be life, and be without flavor - Eleanor Roosevelt",
"In the end, it’s not the years in your life that count. It’s the life in your years - Abraham Lincoln",
"Success usually comes to those who are too busy to be looking for it - Henry David Thoreau",
"Hardships often prepare ordinary people for an extraordinary destiny - C.S. Lewis",
"What lies behind us and what lies before us are tiny matters compared to what lies within us - Ralph Waldo Emerson",
"The only way to do great work is to love what you do - Steve Jobs",
"Believe in yourself and all that you are. Know that there is something inside you that is greater than any obstacle - Christian D. Larson",
"Perfection is not attainable, but if we chase perfection we can catch excellence - Vince Lombardi",
"Life is what happens when you're busy making other plans - John Lennon",
"When we strive to become better than we are, everything around us becomes better too - Paulo Coelho",
"I can't change the direction of the wind, but I can adjust my sails to always reach my destination - Jimmy Dean",
"Life is 10% what happens to us and 90% how we react to it - Charles R. Swindoll",
"You are never too old to set another goal or to dream a new dream - C.S. Lewis",
"A journey of a thousand miles begins with a single step - Lao Tzu",
"Happiness is not a goal...it's a by-product of a life well-lived - Eleanor Roosevelt",
"In three words I can sum up everything I've learned about life: it goes on - Robert Frost",
"You only live once, but if you do it right, once is enough - Mae West",
"The best time to plant a tree was 20 years ago. The second best time is now - Chinese Proverb",
"Do not wait to strike till the iron is hot; but make it hot by striking - William Butler Yeats",
"The biggest adventure you can take is to live the life of your dreams - Oprah Winfrey",
"If you can dream it, you can achieve it - Zig Ziglar",
"The best way out is always through - Robert Frost",
"The only person you are destined to become is the person you decide to be - Ralph Waldo Emerson",
"The greatest pleasure in life is doing what people say you cannot do - Walter Bagehot",
"Success is not how high you have climbed, but how you make a positive difference to the world - Roy T. Bennett",
"Your time is limited, don't waste it living someone else's life - Steve Jobs",
"The best revenge is massive success - Frank Sinatra",
"Do what you can, with what you have, where you are - Theodore Roosevelt",
"You have within you right now, everything you need to deal with whatever the world can throw at you - Brian Tracy",
"The mind is everything. What you think you become - Buddha",
"The only way to achieve the impossible is to believe it is possible - Charles Kingsleigh",
"Don’t let yesterday take up too much of today - Will Rogers",
"What we think, we become - Buddha",
"We must not allow other people's limited perceptions to define us - Virginia Satir",
"If you want to lift yourself up, lift up someone else - Booker T. Washington",
"Start where you are. Use what you have. Do what you can - Arthur Ashe",
"I find that the harder I work, the more luck I seem to have - Thomas Jefferson",
"Life is short, and it is up to you to make it sweet - Sarah Louise Delany",
"When the going gets tough, the tough get going - Joseph Kennedy",
"When you cease to dream you cease to live - Malcolm Forbes",
"You can't go back and change the beginning, but you can start where you are and change the ending - C.S. Lewis",
"Your life does not get better by chance, it gets better by change - Jim Rohn",
"The harder the conflict, the more glorious the triumph - Thomas Paine",
"You are enough just as you are - Meghan Markle",
"Every moment is a fresh beginning - T.S. Eliot",
"The unexamined life is not worth living - Socrates",
"Dream big and dare to fail - Norman Vaughan",
"Turn your wounds into wisdom - Oprah Winfrey",
"Don't wait. The time will never be just right - Napoleon Hill",
"You get what you give - Jennifer Lopez",
"Tough times never last, but tough people do - Robert H. Schuller",
"If you do what you’ve always done, you’ll get what you’ve always gotten - Tony Robbins",
"The secret of getting ahead is getting started - Mark Twain",
"Only those who dare to fail greatly can ever achieve greatly - Robert F. Kennedy",
"Keep your eyes on the stars and your feet on the ground - Theodore Roosevelt",
"Success is not in what you have, but who you are - Bo Bennett",
"Do not follow where the path may lead. Go instead where there is no path and leave a trail - Ralph Waldo Emerson",
"The best way to appreciate your job is to imagine yourself without one - Oscar Wilde",
"Change your life today. Don’t gamble on the future, act now, without delay - Simone de Beauvoir",
"Your worth consists in what you are and not in what you have - Thomas Edison",
"Don't be pushed around by the fears in your mind. Be led by the dreams in your heart - Roy T. Bennett",
"Every man must learn this simple truth: Pain is inevitable and suffering is optional",
"In the heat of battle, the fog of war, under pressure,   The undisciplined die "
];

    
    function displayDailyQuote() {
        const randomIndex = Math.floor(Math.random()*quotes.length);
        const quote = quotes[randomIndex];
        document.getElementById('daily-quote').innerText = quote;
    }

    
    displayDailyQuote();
</script>
